{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will focus primarily on modelling, various types of Univariate Models.\n",
    "\n",
    "<b>Interesting Read : </b>[Forecasting Principles & Practices](https://otexts.com/fpp2/)\n",
    "\n",
    "<img src='../Materials/fpp2.png' width='250' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.117719Z",
     "start_time": "2020-12-06T17:15:48.117106Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from cycler import cycler\n",
    "\n",
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Viualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mplfinance as mpl\n",
    "\n",
    "# Time Series Specific\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Dataholder\n",
    "from helperhandler import dataHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Path and Variable Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.122953Z",
     "start_time": "2020-12-06T17:15:49.120549Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_path = '../'\n",
    "raw_datapath = root_path+'Raw Data/'\n",
    "prepared_datapath = root_path+'Prepared Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.127304Z",
     "start_time": "2020-12-06T17:15:49.124653Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (15,7)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Splitting Data\n",
    "\n",
    "There is a need to split the data such that we land with three buckets of the Data:-\n",
    "\n",
    "- Train, CV & Test Splits\n",
    "    - Training Data - Data that the Model Learns upon\n",
    "    - Cross Validation Data - Data on which we determine our hyperparameters/parameters of the model on\n",
    "    - Test Data - Data on which we measure our `Metrics` as to how the model is doing on a dataset which it has never seen before, neither in Training or in Cross Validation Phase.\n",
    "\n",
    "<img src='../Materials/SimpleTrainTestSplit.png'>\n",
    "\n",
    "****\n",
    "- Rolling Splits\n",
    "\n",
    "Usually time series models that are built, are statistical based and in some cases mahine learning or deep learning based, but either ways these models are not that heavy that re-reunning these models every month to generate forecasts will be Computationally expensive, or atleast that is and has been true for most of the cases.\n",
    "\n",
    "<img src='../Materials/SimpleRollingForecast.png' align='left' width='460'>\n",
    "<img src='../Materials/SimpleRollingForecastWithStepAhead.png' align='left' width='460'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Credit : https://otexts.com/fpp2/accuracy.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.134660Z",
     "start_time": "2020-12-06T17:15:49.129142Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generat_randomtsdata(tstart = '1990-01-01', tend = '2020-01-01', plot=False):\n",
    "    rows = (pd.to_datetime(tend)-pd.to_datetime(tstart)).days+1\n",
    "    simData = pd.DataFrame(columns=['Trend',  'Cyclicity', 'Seasonality', 'Residual'],\n",
    "                           index=pd.date_range(tstart, tend, freq='1D'))\n",
    "    \n",
    "    _days=np.arange(rows)\n",
    "\n",
    "    # Trend Componet\n",
    "    simData['Trend'] = 2*np.arange(rows)/rows\n",
    "    # Cyclicity Componet\n",
    "    simData['Cyclicity'] = 0.3*np.sin(3*_days/rows * 2 * np.pi)\n",
    "    # Seasonality - For Additive\n",
    "    simData['Seasonality'] = 0.3*np.sin(_days/365 * 2 * np.pi)\n",
    "    # Seasonality - For Multiplicative\n",
    "    simData['Seasonality'] = 0.3*(_days/rows)*np.sin(_days/365 * 2 * np.pi)\n",
    "    # Residual\n",
    "    simData['Residual'] = np.random.random(rows)/10\n",
    "    # Time Series\n",
    "    simData['ts'] = simData.sum(axis=1)\n",
    "    \n",
    "    if plot : get_decompose_plot(simData)\n",
    "    \n",
    "    return simData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.160347Z",
     "start_time": "2020-12-06T17:15:49.136268Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simData = generat_randomtsdata()\n",
    "simData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>TCT Split</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.165228Z",
     "start_time": "2020-12-06T17:15:49.162633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Ratios\n",
    "training_ratio = 0.8 \n",
    "cv_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.174663Z",
     "start_time": "2020-12-06T17:15:49.169092Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training = int(training_ratio*simData.shape[0])\n",
    "cv = int(cv_ratio*simData.shape[0])\n",
    "test = simData.shape[0]-training-cv\n",
    "\n",
    "training_data = simData.iloc[:training, :]\n",
    "cv_data = simData.iloc[training:training+cv, :]\n",
    "test_data = simData.iloc[-test:, :]\n",
    "\n",
    "training_data.shape, cv_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.181833Z",
     "start_time": "2020-12-06T17:15:49.178140Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Training Data ranges from {0} to {1}'.format(training_data.index.min(),\n",
    "                                                    training_data.index.max()))\n",
    "print('Cross Validation Data ranges from {0} to {1}'.format(cv_data.index.min(),\n",
    "                                                            cv_data.index.max()))\n",
    "print('Testing Data ranges from {0} to {1}'.format(test_data.index.min(),\n",
    "                                                   test_data.index.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Rolling Origin Framework</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.185357Z",
     "start_time": "2020-12-06T17:15:49.183292Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "test_start = '2014-01-01'\n",
    "ahead = 1\n",
    "dfreq = 'D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.193450Z",
     "start_time": "2020-12-06T17:15:49.187554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_cv_data = simData[simData.index<test_start]\n",
    "test_data  = simData[simData.index>=test_start]\n",
    "\n",
    "print('Training-CV Data ranges from {0} to {1}'.format(train_cv_data.index.min(),\n",
    "                                                       train_cv_data.index.max()))\n",
    "print('Testing Data ranges from {0} to {1}'.format(test_data.index.min(),\n",
    "                                                   test_data.index.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.458915Z",
     "start_time": "2020-12-06T17:15:49.195226Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.469180Z",
     "start_time": "2020-12-06T17:15:49.460891Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_model(traincv_data, test_data, \n",
    "                cv_window, ahead_offest,\n",
    "                target_col, feature_cols,\n",
    "                metric_func, metric='MAPE',):\n",
    "    \n",
    "    traincv_data.sort_index(inplace=True)\n",
    "    cvDF = pd.DataFrame(columns = ['Actual', 'Forecast', metric])\n",
    "    testDF = pd.DataFrame(columns=['Actual', 'Forecast', metric])\n",
    "    \n",
    "    feat_cols = feature_cols\n",
    "    model = None\n",
    "\n",
    "    # Prepare Dates\n",
    "    tstart_date = traincv_data.index[0]\n",
    "    for tend_date in train_cv_data.index[-cv_window:-ahead_offest.days]:\n",
    "        point_date = tend_date+ahead_offest\n",
    "\n",
    "#         #print('Training Start Date : ', tstart_date.strftime('%Y-%m-%d'))\n",
    "#         #print('Training End Date : ', tend_date.strftime('%Y-%m-%d'))\n",
    "#         #print('Point Forecast Date : ', point_date.strftime('%Y-%m-%d')+'\\n\\n')\n",
    "        \n",
    "        # Filter the data\n",
    "        _train_data = train_cv_data.truncate(before=tstart_date, after=tend_date)\n",
    "        _cv_data = train_cv_data.loc[point_date].to_frame().T\n",
    "        # Get X & Y\n",
    "        _train_dataX = _train_data[feat_cols]\n",
    "        _train_dataY = _train_data[target_col]\n",
    "        \n",
    "        _cv_dataX = _cv_data[feat_cols]\n",
    "        _cv_dataY = _cv_data[target_col]\n",
    "        \n",
    "        \n",
    "        # Fit your model on the _train_data\n",
    "        # -------------FILL--------------\n",
    "        model = LinearRegression()\n",
    "        # Test your model on the _cv_data\n",
    "        # -------------FILL--------------\n",
    "        model.fit(_train_dataX, _train_dataY)\n",
    "        # Calculate the metric for _cv_data\n",
    "        # forecast and fill cvDF\n",
    "        # -------------FILL--------------\n",
    "        _forecast = model.predict(_cv_dataX)[0]\n",
    "        _actual = _cv_dataY.values[0]\n",
    "\n",
    "        cvDF.loc[point_date, 'Actual'] = _actual\n",
    "        cvDF.loc[point_date, 'Forecast'] = _forecast\n",
    "        cvDF.loc[point_date,  metric] = metric_func(_actual, _forecast)\n",
    "\n",
    "    # Forecast on the Test Data\n",
    "    _test_dataX = test_data[feat_cols]\n",
    "    _test_dataY = test_data[target_col]\n",
    "    testDF['Actual'] = _test_dataY\n",
    "    testDF['Forecast'] = model.predict(_test_dataX)\n",
    "    testDF[metric] = testDF.apply(lambda x : metric_func(x.Actual, x.Forecast), axis=1)\n",
    "    \n",
    "    return cvDF, testDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.877023Z",
     "start_time": "2020-12-06T17:15:49.471393Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mape_func = lambda y,yhat: np.round(100*(abs(y-yhat)/y),2)\n",
    "\n",
    "\n",
    "res = train_model(traincv_data = train_cv_data, test_data = test_data,\n",
    "                  cv_window = 50, ahead_offest = pd.DateOffset(days=3),\n",
    "                  feature_cols = ['Trend', 'Seasonality'], target_col = 'ts',\n",
    "                  metric='MAPE(%)', metric_func=mape_func)\n",
    "\n",
    "cv_metric_sheet, test_metric_sheet = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.881493Z",
     "start_time": "2020-12-06T17:15:49.878943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp = lambda x,y : print(x+y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.887471Z",
     "start_time": "2020-12-06T17:15:49.884256Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.897662Z",
     "start_time": "2020-12-06T17:15:49.889006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cv_metric_sheet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.909674Z",
     "start_time": "2020-12-06T17:15:49.900341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_metric_sheet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.919190Z",
     "start_time": "2020-12-06T17:15:49.912620Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('MAPE for Cross Validation : ', np.round(cv_metric_sheet.iloc[:,-1].mean(),3))\n",
    "print('MAPE for Test Data        : ', np.round(test_metric_sheet.iloc[:,-1].mean(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Residual Diagnostics\n",
    "\n",
    "Residual in a Time Series Model is what is left over after the Forecast, i.e `residual=Actual-Forecast`\n",
    "\n",
    "Residuals are useful in checking whether a model has adequately captured the information in the data. A good forecasting method will yield residuals with the following properties:\n",
    "\n",
    "- The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.\n",
    "- The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.\n",
    "- The residuals have constant variance.\n",
    "- The residuals are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:49.927098Z",
     "start_time": "2020-12-06T17:15:49.921554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "def residual_diagnostic(metric_sheet):\n",
    "    metric_sheet=metric_sheet.copy()\n",
    "    metric_sheet['Residuals'] = metric_sheet.Actual-metric_sheet.Forecast\n",
    "    \n",
    "    grid = plt.GridSpec(2, 2, wspace=0.1, hspace=0.2)\n",
    "    series_ax = plt.subplot(grid[0:1, :])\n",
    "    series_ax.set_title('Residuals')\n",
    "    metric_sheet['Residuals'].plot(ax=series_ax)\n",
    "\n",
    "    dist_ax = plt.subplot(grid[1, 0])\n",
    "    dist_ax.set_title('Residual Distribution')\n",
    "    sns.distplot(metric_sheet['Residuals'], ax=dist_ax, rug=True, rug_kws={'color':'r'})\n",
    "\n",
    "    acf_ax = plt.subplot(grid[1, 1])\n",
    "    acf_ax.set_title('Auto-Correlation')\n",
    "    _=plot_acf(metric_sheet['Residuals'], ax=acf_ax)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:50.521757Z",
     "start_time": "2020-12-06T17:15:49.928877Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "residual_diagnostic(cv_metric_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Univariate Models\n",
    "\n",
    "- Average Model : Taking Average of the Available Data\n",
    "- N채ive Model : The Prior Value is your next Vaue\n",
    "- Seasonal N채ive Model : Give the Same Value as Prior Season, i.e Forecast for Jan-2020 will be the value of Jan-2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Average Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:50.530314Z",
     "start_time": "2020-12-06T17:15:50.523621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_model(traincv_data, test_data, \n",
    "                cv_window, ahead_offest,\n",
    "                target_col,\n",
    "                metric_func, metric='MAPE',):\n",
    "    \n",
    "    comb_data = pd.concat([traincv_data, test_data])\n",
    "    traincv_data.sort_index(inplace=True)\n",
    "    cvDF = pd.DataFrame(columns = ['Actual', 'Forecast', metric])\n",
    "    testDF = pd.DataFrame(columns=['Actual', 'Forecast', metric])\n",
    "    \n",
    "    # Prepare Dates\n",
    "    tstart_date = traincv_data.index[0]\n",
    "    for tend_date in traincv_data.index[-cv_window:-ahead_offest.days]:\n",
    "        point_date = tend_date+ahead_offest\n",
    "        \n",
    "        # Filter the data\n",
    "        _train_data = traincv_data.truncate(before=tstart_date, after=tend_date)\n",
    "        _cv_data = traincv_data.loc[point_date].to_frame().T\n",
    "\n",
    "        _train_dataY = _train_data[target_col]\n",
    "        _cv_dataY = _cv_data[target_col]\n",
    "\n",
    "        _forecast = _train_dataY.mean()\n",
    "        _actual = _cv_dataY.values[0]\n",
    "\n",
    "        cvDF.loc[point_date, 'Actual'] = _actual\n",
    "        cvDF.loc[point_date, 'Forecast'] = _forecast\n",
    "        cvDF.loc[point_date,  metric] = metric_func(_actual, _forecast)\n",
    "\n",
    "    # Forecast on the Test Data\n",
    "    _test_dataY = test_data[target_col]\n",
    "    testDF['Actual'] = _test_dataY\n",
    "    testDF['Forecast'] = comb_data.expanding().mean()\n",
    "    testDF[metric] = testDF.apply(lambda x : metric_func(x.Actual, x.Forecast), axis=1)\n",
    "    \n",
    "    return cvDF, testDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:52.253805Z",
     "start_time": "2020-12-06T17:15:50.532101Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mape_func=lambda y,yhat: np.round(100*(abs(y-yhat)/y),2)\n",
    "\n",
    "\n",
    "res = train_model(traincv_data = train_cv_data, test_data = test_data,\n",
    "                  cv_window = 1000, ahead_offest = pd.DateOffset(days=3),\n",
    "                  target_col = 'ts',\n",
    "                  metric='MAPE(%)', metric_func=mape_func)\n",
    "\n",
    "avg_cvmsheet, avg_tmsheet = res\n",
    "print('MAPE for Cross Validation : ', np.round(avg_cvmsheet.iloc[:,-1].mean(),3))\n",
    "print('MAPE for Test Data        : ', np.round(avg_tmsheet.iloc[:,-1].mean(),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>N채ive Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:52.263370Z",
     "start_time": "2020-12-06T17:15:52.256972Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_model(traincv_data, test_data, \n",
    "                cv_window, ahead_offest,\n",
    "                target_col,\n",
    "                metric_func, metric='MAPE'):\n",
    "    \n",
    "    comb_data = pd.concat([traincv_data, test_data])\n",
    "    traincv_data.sort_index(inplace=True)\n",
    "    cvDF = pd.DataFrame(columns = ['Actual', 'Forecast', metric])\n",
    "    testDF = pd.DataFrame(columns=['Actual', 'Forecast', metric])\n",
    "    \n",
    "    # Prepare Dates\n",
    "    tstart_date = traincv_data.index[0]\n",
    "    for tend_date in traincv_data.index[-cv_window:-ahead_offest.days]:\n",
    "        point_date = tend_date+ahead_offest\n",
    "        \n",
    "        # Filter the data\n",
    "        _train_data = traincv_data.truncate(before=tstart_date, after=tend_date)\n",
    "        _cv_data = traincv_data.loc[point_date].to_frame().T\n",
    "        \n",
    "        _train_dataY = _train_data[target_col]\n",
    "        _cv_dataY = _cv_data[target_col]\n",
    "\n",
    "        _forecast = _train_dataY[-1]\n",
    "        _actual = _cv_dataY.values[0]\n",
    "\n",
    "        cvDF.loc[point_date, 'Actual'] = _actual\n",
    "        cvDF.loc[point_date, 'Forecast'] = _forecast\n",
    "        cvDF.loc[point_date,  metric] = metric_func(_actual, _forecast)\n",
    "\n",
    "    # Forecast on the Test Data\n",
    "    _prev_dt = test_data.index-ahead_offest\n",
    "    _test_dataY = test_data[target_col]\n",
    "    testDF['Actual'] = _test_dataY\n",
    "    testDF['Forecast'] = comb_data.truncate(_prev_dt.min(), _prev_dt.max())[target_col].values\n",
    "    testDF[metric] = testDF.apply(lambda x : metric_func(x.Actual, x.Forecast), axis=1)\n",
    "    \n",
    "    return cvDF, testDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:53.805790Z",
     "start_time": "2020-12-06T17:15:52.265158Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mape_func=lambda y,yhat: np.round(100*(abs(y-yhat)/y),2)\n",
    "\n",
    "res = train_model(traincv_data = train_cv_data, test_data = test_data,\n",
    "                  cv_window = 1000, ahead_offest = pd.DateOffset(days=3),\n",
    "                  target_col = 'ts',\n",
    "                  metric='MAPE(%)', metric_func=mape_func)\n",
    "\n",
    "naive_cvmsheet, naive_tmsheet = res\n",
    "print('MAPE for Cross Validation : ', np.round(naive_cvmsheet.iloc[:,-1].mean(),3))\n",
    "print('MAPE for Test Data        : ', np.round(naive_tmsheet.iloc[:,-1].mean(),3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Seasonal N채ive Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:53.819835Z",
     "start_time": "2020-12-06T17:15:53.813752Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_model(traincv_data, test_data, \n",
    "                cv_window, ahead_offest,\n",
    "                target_col,\n",
    "                metric_func, metric='MAPE',):\n",
    "    \n",
    "    comb_data = pd.concat([traincv_data, test_data])\n",
    "    traincv_data.sort_index(inplace=True)\n",
    "    cvDF = pd.DataFrame(columns = ['Actual', 'Forecast', metric])\n",
    "    testDF = pd.DataFrame(columns=['Actual', 'Forecast', metric])\n",
    "    \n",
    "    # Prepare Dates\n",
    "    tstart_date = traincv_data.index[0]\n",
    "    for tend_date in traincv_data.index[-cv_window:-ahead_offest.days]:\n",
    "        point_date = tend_date+ahead_offest\n",
    "\n",
    "        # Filter the data\n",
    "        _train_data = traincv_data.loc[point_date-pd.DateOffset(months=12)]\n",
    "        \n",
    "        _cv_data = traincv_data.loc[point_date].to_frame().T\n",
    "        \n",
    "        _cv_dataY = _cv_data[target_col]\n",
    "\n",
    "        _forecast = _train_data[target_col]\n",
    "        _actual = _cv_dataY.values[0]\n",
    "\n",
    "        cvDF.loc[point_date, 'Actual'] = _actual\n",
    "        cvDF.loc[point_date, 'Forecast'] = _forecast\n",
    "        cvDF.loc[point_date,  metric] = metric_func(_actual, _forecast)\n",
    "\n",
    "    #Forecast on the Test Data\n",
    "    _prev_seasondt = test_data.index-pd.DateOffset(months=12)\n",
    "    \n",
    "    _test_dataY = test_data[target_col]\n",
    "    testDF['Actual'] = _test_dataY\n",
    "    testDF['Forecast'] = comb_data.truncate(_prev_seasondt.min(), _prev_seasondt.max())[target_col].values\n",
    "    testDF[metric] = testDF.apply(lambda x : metric_func(x.Actual, x.Forecast), axis=1)\n",
    "    \n",
    "    return cvDF, testDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:55.271606Z",
     "start_time": "2020-12-06T17:15:53.824987Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mape_func=lambda y,yhat: np.round(100*(abs(y-yhat)/y),2)\n",
    "\n",
    "res = train_model(traincv_data = train_cv_data, test_data = test_data,\n",
    "                  cv_window = 1000, ahead_offest = pd.DateOffset(days=3),\n",
    "                  target_col = 'ts',\n",
    "                  metric='MAPE(%)', metric_func=mape_func)\n",
    "\n",
    "snaive_cvmsheet, snaive_tmsheet = res\n",
    "print('MAPE for Cross Validation : ', np.round(snaive_cvmsheet.iloc[:,-1].mean(),3))\n",
    "print('MAPE for Test Data        : ', np.round(snaive_tmsheet.iloc[:,-1].mean(),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:55.308860Z",
     "start_time": "2020-12-06T17:15:55.274529Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "combined_msheet = simData[['ts']].copy()\n",
    "combined_msheet.columns = ['Actual']\n",
    "\n",
    "combined_msheet['Average CV'] = avg_cvmsheet.Forecast\n",
    "combined_msheet['Average Test'] = avg_tmsheet.Forecast\n",
    "combined_msheet['Average UL'] = avg_tmsheet.Forecast+1.96*avg_tmsheet.Forecast.expanding().var()\n",
    "combined_msheet['Average LL'] = avg_tmsheet.Forecast-1.96*avg_tmsheet.Forecast.expanding().var()\n",
    "\n",
    "combined_msheet['Naive CV'] = naive_cvmsheet.Forecast\n",
    "combined_msheet['Naive Test'] = naive_tmsheet.Forecast\n",
    "combined_msheet['Naive UL'] = naive_tmsheet.Forecast+1.96*naive_tmsheet.Forecast.expanding().var()\n",
    "combined_msheet['Naive LL'] = naive_tmsheet.Forecast-1.96*naive_tmsheet.Forecast.expanding().var()\n",
    "\n",
    "combined_msheet['Seasonal Naive CV'] = snaive_cvmsheet.Forecast\n",
    "combined_msheet['Seasonal Naive Test'] = snaive_tmsheet.Forecast\n",
    "combined_msheet['Seasonal Naive UL'] = snaive_tmsheet.Forecast+1.96*snaive_tmsheet.Forecast.expanding().var()\n",
    "combined_msheet['Seasonal Naive LL'] = snaive_tmsheet.Forecast-1.96*snaive_tmsheet.Forecast.expanding().var()\n",
    "combined_msheet.head()\n",
    "\n",
    "# combined_msheet = combined_msheet.iloc[-4000:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:56.955955Z",
     "start_time": "2020-12-06T17:15:55.310501Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,1, figsize=(15,12))\n",
    "\n",
    "# Plot the Actual Series\n",
    "combined_msheet[['Actual', 'Average CV', 'Average Test']].plot(ax=axes[0],\n",
    "                                                           title='Average Model')\n",
    "axes[0].fill_between(combined_msheet['Average UL'].dropna().index,\n",
    "                     combined_msheet['Average UL'].dropna(),\n",
    "                     combined_msheet['Average LL'].dropna(), color='g')\n",
    "# Plot for Naive Model\n",
    "combined_msheet[['Actual', 'Naive CV', 'Naive Test']].plot(ax=axes[1],\n",
    "                                                           title='Naive Model')\n",
    "axes[1].fill_between(combined_msheet['Naive UL'].dropna().index,\n",
    "                     combined_msheet['Naive UL'].dropna(),\n",
    "                     combined_msheet['Naive LL'].dropna(), color='g')\n",
    "# Plot for Seasonal Naive Model\n",
    "combined_msheet[['Actual', 'Seasonal Naive CV', 'Seasonal Naive Test']].plot(ax=axes[2],\n",
    "                                                                             title='Seasonal Naive Model')\n",
    "axes[2].fill_between(combined_msheet['Seasonal Naive UL'].dropna().index,\n",
    "                     combined_msheet['Seasonal Naive UL'].dropna(),\n",
    "                     combined_msheet['Seasonal Naive LL'].dropna(), color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# AR Model\n",
    "\n",
    "**Auto Regressive Model** - Model Explaining a process where the current Time Series $Y_{t}$ can be described as a process being governed by its own lags $Y_{t-i}$ $\\Rightarrow$  Predict the time series based on the values that variable assumed in the past..\n",
    "\n",
    "A process being governed as an $AR(p)$ Model can be written as :-\n",
    "\n",
    "\\begin{equation}\n",
    "    AR(p)\\Rightarrow Y_{t} = \\beta_{0} + \\beta_{1}Y_{t-1} + \\beta_{2}Y_{t-2} + .... + \\beta_{1}Y_{t-p} + \\varepsilon_{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat Y_{t} = \\beta_{0} + \\beta_{1}Y_{t-1} + \\beta_{2}Y_{t-2} + .... + \\beta_{1}Y_{t-p}\n",
    "\\end{equation}\n",
    "\n",
    "For ex : \n",
    "\\begin{equation}\n",
    "    AR(1)\\Rightarrow Y_{t} = \\beta_{0} + \\beta_{1}Y_{t-1} + \\varepsilon_{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat Y_{t} = \\beta_{0} + \\beta_{1}Y_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "Where : \n",
    "- $Y_{t}$ equation modelling the data generation process\n",
    "- $\\hat Y_{t}$ equation modelling the forecast, you will notice that the $\\varepsilon_{t}$ is missing from this equation, thats because in that month we dont have access to that error yet.\n",
    "\n",
    "Basically, the interpretation of the above model processes will go like this $\\Rightarrow$ \n",
    "\n",
    "My current month value $Y_{t}$ can be `described` or `regressed` by $Y_{t-1}$, $Y_{t-2}$ ... $Y_{t-p}$, and the error term $\\varepsilon_{t}$ that reflects how `off` our prediction of the current month might be..\n",
    "\n",
    "\n",
    "For AR Model:\n",
    "\n",
    "- `PACF` plot can hint us as to which lags are significant for us to consider regressing upon\n",
    "- `ACF` plot will show a gradual decay in the plot with the increasing lags being engulfed by the error bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Generating the Data for $AR(1)$</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:56.961173Z",
     "start_time": "2020-12-06T17:15:56.958198Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:56.966257Z",
     "start_time": "2020-12-06T17:15:56.963174Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ar1_datagen(beta0=1, beta1= -0.8, size=10_00):\n",
    "    gendata = [beta0]\n",
    "    errors = np.random.normal(size=size)\n",
    "    for i in range(1, size):\n",
    "        y_t = beta0 + beta1*gendata[i-1] + errors[i]\n",
    "        gendata.append(y_t)\n",
    "    return np.array(gendata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:57.225179Z",
     "start_time": "2020-12-06T17:15:56.967996Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_beta1 = 0.9\n",
    "_beta2 = -0.9\n",
    "\n",
    "ar1_bp = ar1_datagen(beta1=_beta1)\n",
    "ar1_bn = ar1_datagen(beta1=-_beta2)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "axes[0].plot(ar1_bp)\n",
    "axes[0].set_title(f'$ beta {1}={_beta1}')\n",
    "axes[1].plot(ar1_bn)\n",
    "_=axes[1].set_title(f'$ beta {1}={_beta2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:57.802642Z",
     "start_time": "2020-12-06T17:15:57.228190Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "_=plot_acf(ar1_bp, ax=axes[0], title='Autocorrelation for Positive beta1')\n",
    "_=plot_pacf(ar1_bp, ax=axes[1], title='Partial Autocorrelation for Positive beta1')\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "_=plot_acf(ar1_bn, ax=axes[0], title='Autocorrelation for Negative beta1')\n",
    "_=plot_pacf(ar1_bn, ax=axes[1], title='Partial Autocorrelation for Negative beta1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MA Model\n",
    "\n",
    "*NOTE : DO NOT CONFUSE THE TERM MOVING AVERAGE WITH THE `MOVING AVERAGE` FROM THE SMOOTHING OPERATIONS, you can interpret it more like moving around the averages*\n",
    "\n",
    "**Moving Average Model** - Model Explaining a process where the current Time Series $Y_{t}$ can be described as a process being governed by its previous forecasting $\\varepsilon_{t-i}$ errors and the $\\mu$\n",
    "\n",
    "A process being governed as an $MA(q)$ Model can be written as :-\n",
    "\n",
    "\\begin{equation}\n",
    "    MA(q)\\Rightarrow Y_{t} = \\mu + \\phi_{1}\\varepsilon_{t-1} + \\phi_{2}\\varepsilon_{t-2} + .... + \\phi_{1}\\varepsilon_{t-q} + \\varepsilon_{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat Y_{t} = \\mu + \\phi_{1}\\varepsilon_{t-1} + \\phi_{2}\\varepsilon_{t-2} + .... + \\phi_{1}\\varepsilon_{t-p}\n",
    "\\end{equation}\n",
    "\n",
    "For ex : \n",
    "\\begin{equation}\n",
    "    MA(1)\\Rightarrow Y_{t} = \\mu + \\phi_{1}\\varepsilon_{t-1} + \\varepsilon_{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat Y_{t} = \\mu + \\phi_{1}\\varepsilon_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "Where : \n",
    "- $Y_{t}$ equation modelling the data generation process\n",
    "- $\\hat Y_{t}$ equation modelling the forecast, you will notice that the $\\varepsilon_{t}$ is missing from this equation, thats because in that month we dont have access to that error yet.\n",
    "\n",
    "Basically, the interpretation of the above model processes will go like this $\\Rightarrow$ \n",
    "\n",
    "My current month value $Y_{t}$ can be `described` or `regressed` by $\\varepsilon_{t-1}$, $\\varepsilon_{t-2}$ ... $\\varepsilon_{t-p}$, and the error term $\\varepsilon_{t}$ that reflects how `off` our prediction of the current month might be..\n",
    "\n",
    "\n",
    "For MA Model:\n",
    "\n",
    "- `ACF` plot can hint us as to which lags are significant\n",
    "- `ACF` plot will show an abrupt shut down in the values, which might be right after the $q$ lag\n",
    "\n",
    "[Should i use MA or AR?](https://stats.stackexchange.com/a/107855)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:57.807174Z",
     "start_time": "2020-12-06T17:15:57.804755Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:57.812149Z",
     "start_time": "2020-12-06T17:15:57.808914Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ma1_datagen(mu=20, phi1= -0.8, size=1_00):\n",
    "    gendata = [mu]\n",
    "    errors = np.random.randn(size)\n",
    "    for i in range(1, size):\n",
    "        y_t = mu+phi1*errors[i-1] + errors[i]\n",
    "        gendata.append(y_t)\n",
    "    return np.array(gendata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:58.214684Z",
     "start_time": "2020-12-06T17:15:57.813839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ma1_p_8 = ma1_datagen(phi1=0.5)\n",
    "ma1_n_8 = ma1_datagen(phi1=-0.5)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "axes[0].plot(ma1_p_8)\n",
    "axes[0].set_title(r'$ \\phi_{1}=0.8$')\n",
    "axes[1].plot(ma1_n_8)\n",
    "_=axes[1].set_title(r'$ \\phi_{1}=-0.8$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:58.678214Z",
     "start_time": "2020-12-06T17:15:58.217145Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "_=plot_acf(ma1_p_8, ax=axes[0])\n",
    "_=plot_pacf(ma1_p_8, ax=axes[1])\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "_=plot_acf(ma1_n_8, ax=axes[0])\n",
    "_=plot_pacf(ma1_n_8, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ARIMA Model\n",
    "\n",
    "**AutoRegressive Integrated Moving Average Model** - Model Explaining a process where the current Time Series $Y_{t}$ can be described as a process being governed by its $MA(q)$ and $AR(p)$ terms, alongside a $d$ parameter which belongs to the differencing of the series.\n",
    "\n",
    "**ARIMA** model is usually advisable when the time series is non stationary and is having a trend, not seasonality.\n",
    "\n",
    "A process being governed as an $ARIMA(p,d,q)$ Model can be written as :-\n",
    "\n",
    "\\begin{equation}\n",
    "    ARIMA(p,d,q)\\Rightarrow Z_{t} = \\mu + \\phi_{1}\\varepsilon_{t-1} + \\phi_{2}\\varepsilon_{t-2} + .... + \\phi_{1}\\varepsilon_{t-q} + \\beta_{0} + \\beta_{1}Y_{t-1} + \\beta_{2}Y_{t-2} + .... + \\beta_{1}Y_{t-p} + \\varepsilon_{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat Z_{t} = \\mu + \\phi_{1}\\varepsilon_{t-1} + \\phi_{2}\\varepsilon_{t-2} + .... + \\phi_{1}\\varepsilon_{t-q} + \\beta_{0} + \\beta_{1}Y_{t-1} + \\beta_{2}Y_{t-2} + .... + \\beta_{1}Y_{t-p}\n",
    "\\end{equation}\n",
    "\n",
    "For ex : \n",
    "\\begin{equation}\n",
    "    ARIMA(1,1,1)\\Rightarrow Z_{t} = \\mu + \\beta_{0} + \\phi_{1}\\varepsilon_{t-1} + \\beta_{1}Y_{t-1} + \\varepsilon_{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat Z_{t} = \\mu + \\beta_{0} + \\phi_{1}\\varepsilon_{t-1} + \\beta_{1}Y_{t-1}\n",
    "\\end{equation}\n",
    "\n",
    "Where : \n",
    "- $Y_{t}$ equation modelling the data generation process\n",
    "- $Z_{t}$ is the differenced $Y_{t}$ series\n",
    "- $\\hat Y_{t}$ equation modelling the forecast, you will notice that the $\\varepsilon_{t}$ is missing from this equation, thats because in that month we dont have access to that error yet.\n",
    "\n",
    "Basically, the interpretation of the above model processes will go like this $\\Rightarrow$ \n",
    "\n",
    "My current month value $Y_{t}$ can be `described` or `regressed` by $\\varepsilon_{t-1}$, $\\varepsilon_{t-2}$ ... $\\varepsilon_{t-p}$, and the error term $\\varepsilon_{t}$ that reflects how `off` our prediction of the current month might be..\n",
    "\n",
    "\n",
    "For MA Model:\n",
    "\n",
    "- `ACF` plot can hint us as to which lags are significant\n",
    "- `ACF` plot will show an abrupt shut down in the values, which might be right after the $q$ lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:58.682759Z",
     "start_time": "2020-12-06T17:15:58.680186Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:58.688072Z",
     "start_time": "2020-12-06T17:15:58.684451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def arima111_datagen(mu=20, phi1= -0.8, beta0=1, beta1=0.8, size=1_00):\n",
    "    gendata = [mu+beta0]\n",
    "    errors = np.random.randn(size)\n",
    "    for i in range(1, size):\n",
    "        y_t = mu+beta0+phi1*errors[i-1] + beta1*gendata[i-1] + errors[i]\n",
    "        gendata.append(y_t)\n",
    "    return np.array(gendata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:58.964664Z",
     "start_time": "2020-12-06T17:15:58.689546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "arima111_1 = arima111_datagen(phi1= 10, beta1=0.8)\n",
    "arima111_2 = arima111_datagen(phi1= -10, beta1=-0.8)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "axes[0].plot(arima111_1)\n",
    "axes[0].set_title(r'$ \\phi_{1}=-0.8 \\beta_{1}=0.8$')\n",
    "axes[1].plot(arima111_2)\n",
    "_=axes[1].set_title(r'$ \\phi_{1}=0.8 \\beta_{1}=-0.8$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:59.202553Z",
     "start_time": "2020-12-06T17:15:58.967007Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "_=plot_acf(arima111_2, ax=axes[0])\n",
    "_=plot_pacf(arima111_2, ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SARIMA Model\n",
    "\n",
    "**Seaonal AutoRegressive Integrated Moving Average Model** - In addition to ARIMA model, this class of model takes into account the **Seasonality** in the dataset. Finally the SARIMA model is defined by the following parameters :\n",
    "\n",
    "- p : Auto-Regressive order of the Series\n",
    "- d : Integrated(Differencing) order of the Series\n",
    "- q : Moving Average order of the Series\n",
    "\n",
    "- P : Auto-Regressive order of the Seasonality\n",
    "- D : Integrated(Differencing order of the Seasonality\n",
    "- Q : Moving Average order of the Seasonality\n",
    "\n",
    "- m : Number of time steps for the period to occur\n",
    "\n",
    "**SARIMA** model is usually advisable when the time series is non stationary and is having a trend and a seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Comparision\n",
    "\n",
    "- Akaike Information Criterion (AIC)\n",
    "\n",
    "\\begin{equation}\n",
    "    AIC = 2k - 2ln(\\hat L)\n",
    "\\end{equation}\n",
    "\n",
    "- Bayesian information criterion (BIC)\n",
    "\n",
    "\\begin{equation}\n",
    "    BIC = k.ln(n) - 2ln(\\hat L)\n",
    "\\end{equation}\n",
    "\n",
    "- [DIC](https://www.youtube.com/watch?v=xS4jDHQfP2o)\n",
    "- [WAIC](https://www.youtube.com/watch?v=xS4jDHQfP2o)\n",
    "\n",
    "Where :- \n",
    "\n",
    "- $k$ - Number of Parameters in the model\n",
    "\n",
    "- $n$ - Total Number of DataPoints\n",
    "\n",
    "- $\\hat L$ - Maximum value of the Likelihood function of the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:59.208244Z",
     "start_time": "2020-12-06T17:15:59.205496Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:59.821420Z",
     "start_time": "2020-12-06T17:15:59.210138Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:15:59.832977Z",
     "start_time": "2020-12-06T17:15:59.823714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mdata = dataHolder.bucket['usa_cpi'].data.copy()\n",
    "mdata.index.freq = pd.infer_freq(mdata.index)\n",
    "\n",
    "mdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:16:00.186608Z",
     "start_time": "2020-12-06T17:15:59.834539Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_=plot_acf(mdata)\n",
    "_=plot_pacf(mdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:16:00.380069Z",
     "start_time": "2020-12-06T17:16:00.188574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.bucket['usa_cpi'].exploratory_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:16:00.447646Z",
     "start_time": "2020-12-06T17:16:00.382323Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1 = ARIMA(mdata.CPI, order=(0,1,3))\n",
    "fit1 = model1.fit()\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T17:16:01.165110Z",
     "start_time": "2020-12-06T17:16:00.449800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model1 = ARIMA(mdata.CPI, order=(4,2,2))\n",
    "fit1 = model1.fit()\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Testing Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "547.4080810546875px",
    "left": "1610.900634765625px",
    "top": "271.4779357910156px",
    "width": "343.2168884277344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will focus primarily statistical side of the time series modelling, including how to define Correlation's, the concept of Stationarity, Auto-Correlation & Partial Auto-Correlation Function, Auto-Regressive & Moving Average Processes in modelling and how to perform Model Diagnostics.\n",
    "\n",
    "<b>Interesting Read : </b>[STAT 510](https://online.stat.psu.edu/stat510/)\n",
    "\n",
    "<img src='../Materials/stat_501.png' width='550' align='left'>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Viualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mplfinance as mpl\n",
    "\n",
    "# Time Series Specific\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "# Datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# DataHandler\n",
    "from helperhandler import dataHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Path and Variable Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_path = '../'\n",
    "raw_datapath = root_path+'Raw Data/'\n",
    "prepared_datapath = root_path+'Prepared Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (15,7)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load & Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.dataDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## USA Consumer, Income, Production, Savings & Unemployment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k='usa_economic'\n",
    "print(dataHolder.bucket[k].long_description)\n",
    "dataHolder.bucket[k].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.bucket[k].exploratory_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Air Passengers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k='airp_data'\n",
    "print(dataHolder.bucket[k].long_description)\n",
    "dataHolder.bucket[k].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.bucket[k].exploratory_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Beer Production Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k='beer_prod'\n",
    "print(dataHolder.bucket[k].long_description)\n",
    "dataHolder.bucket[k].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.bucket[k].exploratory_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Britannia Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k='brit_stock'\n",
    "print(dataHolder.bucket[k].long_description)\n",
    "dataHolder.bucket[k].data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataHolder.bucket[k].exploratory_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Correlation\n",
    "\n",
    "**Correlation** : Two variables are said to be correlated when the value assumed by one affects the distribution of the other. It reflects the association between the two variables whose strength usually lies within the range of -1 to +1. If, as the value of X increase there is an increase in Y, then X & Y are said to be positively correlated. Also if, as the value of X decrease there is an increase in Y, then X & Y are said to be negatively correlated.\n",
    "\n",
    "Different Types Of Correlations\n",
    "\n",
    "<img src='https://miro.medium.com/max/2000/1*cxBhYwEBPLvs0E8E7Ehe_g.png' width='500'>\n",
    "\n",
    "- Pearson Correlation : Quantifying association between Two continuous features.\n",
    "\\begin{equation}\n",
    "    \\rho_{p}=\\frac{Cov(X,Y)}{\\sigma_{x} \\sigma_{y}} = \\frac{\\sum_{i=1}^{n}((x_{i}-\\bar{x})(y_{i}-\\bar{y}))}{\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2 }*{\\sqrt{\\sum_{i=1}^{n}(y_{i}-\\bar{y})^2 }}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- Spearman Correlation : Quantifying `RANK` association between ordinal & continuous features.\n",
    "\\begin{equation}\n",
    "    \\rho_{s}=\\frac{Cov(rank(X),rank(Y))}{\\sigma_{rank(x)} \\sigma_{rank(y)}} = \\frac{\\sum_{i=1}^{n}((rank(x_{i})-rank(\\bar{x}))(rank(y_{i})-rank(\\bar{y}))}{\\sqrt{\\sum_{i=1}^{n}(rank(x_{i})-rank(\\bar{x}))^2 }*{\\sqrt{\\sum_{i=1}^{n}(rank(y_{i})-rank(\\bar{y}))^2 }}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- Kendall Tau Correlation : Quantifying `RANK` association between ordinal & continuous features, works quite well with Non-Normally distributed data.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tau_{k} = \\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{n} signum(x_{i}-x_{j}) signum(y_{i}-y_{j})}{n(n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "- Point Biserial Correlation : Quantifying association between Nominal and Continous Feature.\n",
    "\n",
    "\\begin{equation}\n",
    "    r_{pb} = \\frac{M_{1}-M_{0}}{s_{n}}\\sqrt{\\frac{n_{0}n_{1}}{n_{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "    - Here > M1 : Mean of target across 1 category, ; M2 : Mean of the target across 0 category; sn : Standard Deviation in the data; n0, n1: count of the number of datapoints in each category; n : Total number of datapoints.\n",
    "\n",
    "- Annova eta values : Quantifying association between Two nominal features.\n",
    "\n",
    "[A comparision of the Correlations](https://ag-ds-bubble.medium.com/correlations-pearson-correlation-is-not-one-solution-for-all-25f7220220fe)\n",
    "\n",
    "\n",
    "****\n",
    "\n",
    "But be wary of the correlations though, because `Correlation` doesnt always translation into `Causation`\n",
    "\n",
    "[Spurious Correlations](https://tylervigen.com/spurious-correlations)\n",
    "<img src='../Materials/Spurious Correlation.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, kendalltau, spearmanr, pointbiserialr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "n = 10\n",
    "x=np.arange(n)\n",
    "y=x+np.random.uniform(low=-1,high=1, size=len(x))*2\n",
    "\n",
    "# x = x/np.linalg.norm(x)\n",
    "# y = y/np.linalg.norm(y)\n",
    "\n",
    "_=plt.scatter(x,y)\n",
    "_=plt.plot(x,x,c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "\n",
    "x_deviat = x-x_mean\n",
    "y_deviat = y-y_mean\n",
    "\n",
    "numerator = sum(x_deviat*y_deviat)/(n)\n",
    "denominator = np.sqrt(sum(x_deviat**2)/n)*np.sqrt(sum(y_deviat**2)/n)\n",
    "pearson_corr = numerator/denominator\n",
    "\n",
    "pearson_corr, pearsonr(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.c_[x,y]).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "usa_cipsu_data = dataHolder.bucket['usa_economic'].data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "usa_cipsu_data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pearsonr(usa_cipsu_data.Production, usa_cipsu_data.Unemployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kendalltau(usa_cipsu_data.Production, usa_cipsu_data.Unemployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spearmanr(usa_cipsu_data.Production, usa_cipsu_data.Unemployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generating Correlation Heatmaps\n",
    "corr_df = usa_cipsu_data.corr(method='pearson')\n",
    "corr_df = corr_df.style\n",
    "corr_df = corr_df.background_gradient(cmap=sns.light_palette(\"red\", as_cmap=True))\n",
    "corr_df = corr_df.highlight_max(color='black')\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lagged_data = pd.DataFrame()\n",
    "for elag in range(5):\n",
    "    for ecol in ['Consumption', 'Income']:\n",
    "        lagged_data[ecol+'_{0}'.format(elag)] = usa_cipsu_data['Consumption'].shift(elag)\n",
    "lagged_datacorr=lagged_data.corr()\n",
    "lagged_datacorr = lagged_datacorr[['Income_0', 'Income_1', 'Income_2', 'Income_3', 'Income_4']]\n",
    "lagged_datacorr = lagged_datacorr[lagged_datacorr.index.isin(['Consumption_0','Consumption_1','Consumption_2',\n",
    "                                              'Consumption_3','Consumption_4'])]\n",
    "lagged_datacorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lagged_datacorrst = lagged_datacorr.copy().style\n",
    "lagged_datacorrst = lagged_datacorrst.background_gradient(cmap=sns.light_palette(\"red\", as_cmap=True))\n",
    "lagged_datacorrst = lagged_datacorrst.highlight_max(color='black')\n",
    "lagged_datacorrst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_=sns.heatmap(lagged_datacorr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Autocorrelation\n",
    "\n",
    "Since Time Series, of any Object - being measured, is something which is not `expected` to abruptly shift from its trajectory/course which stems a notion of previous `timestamped` values having some association/correlation with the values in very near future/ in a particular seasonal patter to arise. This concpet is something we can take advantage of for predicting the future.\n",
    "\n",
    "Use of AutoCorrelation can be Multiple :- \n",
    "\n",
    "- Figuring out which model to use\n",
    "- Estimating the parameters of the models\n",
    "- Understanding the Underlying structure of the Time Series\n",
    "\n",
    "****\n",
    "There are two forms of Auto-Correlation that are used in Time Series Analysi\n",
    "\n",
    "- ***Auto-Correlation*** : Auto-Correlation as designed above is something which lets us gauge into the internal structure as to how are the previous datapoints related to the ones that are to come. But the catch here is that, \n",
    "\n",
    "Suppose we are calcluate Auto - Correlation between \n",
    "  \n",
    "1)  AC between $y_{t}$ and $y_{t-1}$  = 0.78\n",
    "\n",
    "2)  AC between $y_{t}$ and $y_{t-2}$ = 0.76\n",
    "\n",
    "And we have the following conversation :- \n",
    "\n",
    "\n",
    "**Me :)** Were we to use both of these variables as predictors, since $y_{t-1}$ is already explaining 78% of variance(correlation, for simplicity) of the $y_{t}$, would it be even helpful to use $y_{t-2}$?\n",
    "\n",
    "**You:)** Well it is having 0.76 correlation, maybe? üßê\n",
    "\n",
    "**Me:)** Ok, then i will reverse my question, if $y_{t-2}$ is having 0.76 correlation, would it be wise for us to even consider the $y_{t-1}$ series?\n",
    "\n",
    "**You:)** Well I dont really know, I understand that the there will be effect of $y_{t-2}$ in the $y_{t-1}$ series but how do i quantify that?\n",
    "\n",
    "**Me:)** PARTIAL AUTO CORRELATION \n",
    "\n",
    "**You:)** ü§©ü§©ü§©ü§©, wait, what is Partial Auto Correlation and how do i quantify it?\n",
    "\n",
    "\n",
    "Getting a value of correlation neglecting the compunding effect of the series..\n",
    "\n",
    "- ***Partial Auto-Correlation*** : \n",
    "\n",
    "**Me:)** Effectively what we need from Partial Auto-Correlation is for us to tell that what is the effect of `Individual Lagged Time Series` $y_{t-n}$ on the current time series $y_{t}$. So this is what we do :-\n",
    "\n",
    "- 1.) Take the series $y_{t-1}$ and  $y_{t}$\n",
    "- 2.) Regress $y_{t-1}$ on  $y_{t}$ and note the Correlation between the two.\n",
    "- 3.) Take the residuals(Whatever $y_{t-1}$ wasnt able to explain of $y_{t}$) of that regression, basically the errors\n",
    "- 4.) Now regress the $y_{t-2}$ on the `errors`, and repeat...\n",
    "\n",
    "[Partial Auto Correlation](https://towardsdatascience.com/understanding-partial-auto-correlation-fa39271146ac)\n",
    "\n",
    "Lets jump to code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Auto-Correlation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import acf, plot_acf, pacf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tempdata = lagged_data[[k for k in lagged_data.columns if 'Con' in k]]\n",
    "tempdata.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_=plot_acf(lagged_data.Consumption_0)\n",
    "acf(lagged_data.Consumption_0)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Partial Auto-Correlation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def calc_pacf(data, lags = 2):\n",
    "    data.name='y'\n",
    "    data = data.to_frame()\n",
    "    pcorr=[]\n",
    "    \n",
    "    if lags==1:\n",
    "        return [1]\n",
    "    elif lags==2:\n",
    "        data['y_L1'] = data.y.shift(1)\n",
    "        return [1, data.corr().values[1][0]]\n",
    "    else:\n",
    "        data['y_L1'] = data.y.shift(1)\n",
    "        pcorr = [1, data.corr().values[1][0]]\n",
    "        \n",
    "        for _l in range(2,lags):\n",
    "            _tlag_col = 'y_L_{0}'.format(_l)\n",
    "            calcData = data.y.to_frame().copy()\n",
    "            calcData[_tlag_col] = calcData.y.shift(_l)\n",
    "            reg_cols = []\n",
    "            for _ll in range(1,_l):\n",
    "                scol = 'y_L_{0}'.format(_ll)\n",
    "                calcData[scol] = calcData.y.shift(_ll)\n",
    "                reg_cols.append(scol)\n",
    "            calcData = calcData.dropna()\n",
    "            \n",
    "            # Fit the first Linear Model on t_i\n",
    "            _model = linear_model.LinearRegression()\n",
    "            _model.fit(calcData[reg_cols], calcData['y'])\n",
    "            _resid1 = calcData['y']-_model.predict(calcData[reg_cols])\n",
    "            # Fit the first Linear Model on t_l\n",
    "            _model = linear_model.LinearRegression()\n",
    "            _model.fit(calcData[reg_cols], calcData[_tlag_col])\n",
    "            _resid2 = calcData[_tlag_col]-_model.predict(calcData[reg_cols])\n",
    "            pcorr.append(pearsonr(_resid1.values, _resid2.values)[0])\n",
    "            \n",
    "        return pcorr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calc_pacf(lagged_data.Consumption_0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_=plot_pacf(lagged_data.Consumption_0, lags=10)\n",
    "pacf(lagged_data.Consumption_0)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stationarity\n",
    "\n",
    "Dealing with Time Series Models come with some assumptions, especially univariate models, in which `Stationarity` is one of the primary requirements.\n",
    "\n",
    "If a Time Series is Stationary, \n",
    "- The Time Series $y_{t}$ has a constant mean, i.e $E[Y_{t}]=const$\n",
    "- The Time Series $y_{t}$ has a constant variance, i.e $Var[Y_{t}]=const$\n",
    "- The Time Series $y_{t}$ doesnt exhibit seasonality.\n",
    "\n",
    "How to detect if a Time Seiries is Stationary?\n",
    "- Visual Inspection of the Time Series Plot\n",
    "- Local & Global Tests\n",
    "- Unit Root Tests - `ADF` Test & `KPSS` Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "airpassengers_data = dataHolder.bucket['airp_data'].data.copy()\n",
    "beerprod_data = dataHolder.bucket['beer_prod'].data.copy()\n",
    "britanniastock_data = dataHolder.bucket['brit_stock'].data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_splot1():\n",
    "    fig, axes = plt.subplots(2,3, figsize=(20,10))\n",
    "    _=usa_cipsu_data.Consumption.plot(ax=axes[0,0])\n",
    "    _=axes[0,0].set_xlabel('')\n",
    "    _=axes[0,0].set_ylabel('US Consumption pct change')\n",
    "    _=usa_cipsu_data.Income.plot(ax=axes[0,1])\n",
    "    _=axes[0,1].set_xlabel('')\n",
    "    _=axes[0,1].set_ylabel('US Income pct change')\n",
    "    _=usa_cipsu_data.Unemployment.plot(ax=axes[0,2])\n",
    "    _=axes[0,2].set_xlabel('')\n",
    "    _=axes[0,2].set_ylabel('US Unemployment pct change')\n",
    "\n",
    "\n",
    "    _=airpassengers_data.Passengers.plot(ax=axes[1,0])\n",
    "    _=axes[1,0].set_xlabel('')\n",
    "    _=axes[1,0].set_ylabel('Passengers')\n",
    "    _=beerprod_data.MBP.plot(ax=axes[1,1])\n",
    "    _=axes[1,1].set_xlabel('')\n",
    "    _=axes[1,1].set_ylabel('Beer Production')\n",
    "    _=britanniastock_data.Volume.plot(ax=axes[1,2])\n",
    "    _=axes[1,2].set_xlabel('')\n",
    "    _=axes[1,2].set_ylabel('Britannia Stock Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_splot1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Of these Time Series which do you think is Stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Unit Root Tests\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "def adf_test(timeseries):\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "           dfoutput['Critical Value (%s)'%key] = value\n",
    "    return dfoutput.to_frame()\n",
    "\n",
    "def kpss_test(timeseries):\n",
    "    kpsstest = kpss(timeseries, regression='c', nlags=\"auto\")\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    return kpss_output.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adf_test(usa_cipsu_data.Consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kpss_test(usa_cipsu_data.Consumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Statistical Tests\n",
    "\n",
    "- Test for Auto Correlation\n",
    "- Test for Seasonality (Strength)\n",
    "- Test for Trend (Strength)\n",
    "- Test for Stationarity\n",
    "- Test for Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.stattools import durbin_watson, jarque_bera\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Quantifying the Seasonality & Trend\n",
    "[Strength of Seasonality & Trend](https://otexts.com/fpp2/seasonal-strength.html#seasonal-strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_decompose_plot(data):\n",
    "    data = data.copy()\n",
    "    data.index.name = ''\n",
    "    grid = plt.GridSpec(4, 2, wspace=0.1, hspace=0.5)\n",
    "    series_ax = plt.subplot(grid[0:2, :])\n",
    "    series_ax.set_title('Time Series')\n",
    "    \n",
    "#     ts_trnsfrm, blambda = boxcox(data['ts'])\n",
    "#     data['ts'] = pd.Series(ts_trnsfrm, name='ts', index=data['ts'].index)\n",
    "    data['ts'].plot(ax=series_ax)\n",
    "\n",
    "    trend_ax = plt.subplot(grid[2, 0])\n",
    "    trend_ax.set_title('Trend Series')\n",
    "    data['Trend'].plot(ax=trend_ax)\n",
    "\n",
    "    cycl_ax = plt.subplot(grid[2, 1])\n",
    "    cycl_ax.set_title('Cyclic Component')\n",
    "    data['Cyclicity'].plot(ax=cycl_ax)\n",
    "    \n",
    "    seas_ax = plt.subplot(grid[3, 0])\n",
    "    seas_ax.set_title('Seasonal Component')\n",
    "    data['Seasonality'].plot(ax=seas_ax)\n",
    "\n",
    "    resid_ax = plt.subplot(grid[3, 1])\n",
    "    resid_ax.set_title('Residuals/Noise')\n",
    "    data['Residual'].plot(ax=resid_ax)\n",
    "        \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tsdata = airpassengers_data.Passengers\n",
    "decomp=seasonal_decompose(tsdata, period=12, extrapolate_trend=True, model='multiplicative')\n",
    "\n",
    "decomData = pd.DataFrame(columns=['Trend',  'Cyclicity', 'Seasonality', 'Residual'],\n",
    "                         index=tsdata.index)\n",
    "\n",
    "tt = decomp.trend\n",
    "st = decomp.seasonal\n",
    "rt = decomp.resid\n",
    "\n",
    "decomData['Trend'] = tt\n",
    "decomData['Seasonality'] = st\n",
    "decomData['Residual'] = rt\n",
    "decomData['ts'] = tsdata\n",
    "\n",
    "get_decompose_plot(decomData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_tmeasure=np.var(rt)/np.var(tt+rt)\n",
    "_tmeasure = max([0, 1-_tmeasure])\n",
    "print('Strength of Trend is : ', round(_tmeasure,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_measure=np.var(rt)/np.var(st+rt)\n",
    "_measure = max([0, 1-_measure])\n",
    "print('Strength of Seasonal is : ', round(_measure,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tests for Auto-Correlation\n",
    "- Durbin Watson Test\n",
    "     - **Null Hypothesis :** There is no serial correlation in the residuals\n",
    "     - **Alternate Hypothesis :** Residuals follow an AR1 process\n",
    "     - **In Simple Terms :** If `p-value`<0.05(or any other significance value) then the auto-correlation is present, else if `p-value`>0.05 auto-correlation is absent.\n",
    "     \n",
    "     \n",
    "     \n",
    "- Ljung-Box Test (More Accurate)\n",
    "     - **Null Hypothesis :** The residuals are independently distributed.\n",
    "     - **Alternate Hypothesis :** The residuals are not independently distributed; they exhibit serial correlation.\n",
    "     - **In Simple Terms :** This tests statistics always results in values ranging in from 0 to 4, if the `test-statistics`=2, then this means that there is no serial autocorrelation, if the `test-statistics` is closer to 0 then it mean there can be `Positive Serial Correlation` or if it is towards 4 it can mean that there might be `Negative Serial Correlation`\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "durbin_watson(britanniastock_data.Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "durbin_watson(usa_cipsu_data.Consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "durbin_watson(beerprod_data.MBP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Ljung-Box Test</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = sm.datasets.sunspots.load_pandas().data\n",
    "res = sm.tsa.ARMA(beerprod_data.MBP, (1,1)).fit(disp=-1)\n",
    "acorr_ljungbox(res.resid, lags=[12], return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = sm.datasets.sunspots.load_pandas().data\n",
    "res = sm.tsa.ARMA(usa_cipsu_data.Consumption, (1,1)).fit(disp=-1)\n",
    "acorr_ljungbox(res.resid, lags=[12], return_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tests For stationarity\n",
    "- [Augmented Dickey Fuller (ADF) Test](https://www.youtube.com/watch?v=1opjnegd_hA)\n",
    "     - **Null Hypothesis :** A unit root is present in a time series sample\n",
    "     - **Alternate Hypothesis :** Depending on which version of the test is used, but is usually stationarity or trend-stationarity.\n",
    "     - **In Simple Terms :** If ADF Test outputs `p-value`>0.05(or Significance level set by you), then the time series is non-stationary\n",
    "     \n",
    "     \n",
    "- Kwiatkowski‚ÄìPhillips‚ÄìSchmidt‚ÄìShin (KPSS) Test\n",
    "     - **Null Hypothesis :** The data is stationary.\n",
    "     - **Alternate Hypothesis :** The data is non-stationary.\n",
    "     - **In Simple Terms :** If KPSS Test outputs `p-value`<0.05(or Significance level set by you), then the time series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adf_test(beerprod_data.MBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kpss_test(beerprod_data.MBP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tests for Distributions\n",
    "\n",
    "- Jarque Bera Test\n",
    "     - **Null Hypothesis :** The data is following a Normal Distribution\n",
    "     - **Alternate Hypothesis :** The data is following some other distribution.\n",
    "     - **In Simple Terms :** If Jarque Bera Test outputs `p-value`<0.05(or Significance level set by you), then the time series is not following a Normal Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def jb_test(timeseries):\n",
    "    dftest = jarque_bera(timeseries)\n",
    "    dfoutput = pd.Series(dftest, index=['JB', 'JBpv', 'skew', 'kurtosis'])\n",
    "    dfoutput = dfoutput.to_frame()\n",
    "    dfoutput.index.name = 'Measure'\n",
    "    dfoutput.columns=['Value']\n",
    "    return dfoutput\n",
    "\n",
    "jb_test(usa_cipsu_data.Consumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "white_noise = np.random.normal(loc=3,size=int(1e5))\n",
    "sns.distplot(white_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "jb_test(white_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Q-Q Plot</b>\n",
    "\n",
    "If the line is straight then the data is following the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_=qqplot(usa_cipsu_data.Consumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Metrics\n",
    "\n",
    "- Mean Absolute Percentage Error (MAPE):\n",
    "\\begin{equation}\n",
    "    MAPE = \\frac{1}{n} \\sum_{i=1}^{n}\\left | \\frac{y_{t}-\\hat y_{t}}{y_{t}} * 100\\right|\n",
    "\\end{equation}\n",
    "- Mean Squared Error (MSE):\n",
    "\\begin{equation}\n",
    "    MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{t}-\\hat y_{t})^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "> *Both MAPE & MSE dont have the sense of direction of the error, just the magnitude*\n",
    "\n",
    "> *Both MAPE & MSE have their magnitude which is a reflectance of the error relative to actual target value*\n",
    "\n",
    "> *MSE Penalises greater error more than the smaller error*\n",
    "\n",
    "> *MAPE will become infinite if the actual data is 0*\n",
    "\n",
    "\n",
    "- Custom Metrics : Based on the KPI's being tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Effect on MSE as the prediction moves farther away from the actual</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "x=[1,2,3,4,10,12]\n",
    "for ex in x:\n",
    "    exp = ex+np.random.normal(scale=0.1,size=100)\n",
    "    plt.scatter(exp, (exp-ex)**2, s=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Effect on MAPE as the prediction moves farther away from the actual</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "x=[1,2,3,4,10,12]\n",
    "for ex in x:\n",
    "    exp = ex+np.random.normal(scale=0.1,size=100)\n",
    "    plt.scatter(exp, 100*(abs(exp-ex)/ex),s=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Calculation of the Metrics<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "ydata = dataHolder.bucket['beer_prod'].data.copy()\n",
    "model=ARIMA(ydata.MBP, (1,0,1))\n",
    "fit=model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ydata['Predicted'] = fit.predict(0,475)\n",
    "ydata.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ydata['Residuals'] = ydata.MBP - ydata.Predicted\n",
    "_=ydata.Residuals.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "resid = ydata.Predicted-ydata.MBP\n",
    "resid_scaled = resid/ydata.Predicted\n",
    "resid_scaledabs = sum(abs(resid_scaled))\n",
    "mape = 100*(resid_scaledabs/ydata.shape[0])\n",
    "mape = round(mape, 3)\n",
    "print('MAPE : ', mape, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "resid = ydata.Predicted-ydata.MBP\n",
    "resid_scaled = resid**2\n",
    "resid_scaledabs = sum(abs(resid_scaled))\n",
    "mse = 100*(resid_scaledabs/ydata.shape[0])\n",
    "mse = round(mse, 3)\n",
    "print('MSE : ', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Testing Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "547.4080810546875px",
    "left": "1610.900634765625px",
    "top": "271.4779357910156px",
    "width": "343.2168884277344px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
